# Multi Modal VQA

## Table of Contents
- [Introduction](#introduction)  
- [Repository Structure](#repository-structure)  
- [Prerequisites](#prerequisites)  
- [Installation](#installation)  
- [Running the Code](#running-the-inference-code)
- [Report](#report)  

## Introduction

In this project, we tackle the task of **Visual Question Answering (VQA)**: given an image and a natural-language question about its contents, our system must produce a single, concise word as the answer. To train and evaluate this model, we use the **Amazon–Berkeley Objects** dataset—augmented with rich item metadata—to assemble a high-quality VQA corpus.

### Key steps in our approach include:

- **Dataset Curation**  
  Leveraging large language models to generate 3 diverse question–answer pairs per image based on both visual content and metadata, while enforcing constraints (e.g., no numerical answers).

- **Baseline Modeling**  
  Establishing a transformer baseline (e.g., BLIP, BLIP2) to validate end-to-end performance on the curated VQA pairs.

- **Fine-Tuning with LoRA**  
  Applying Low-Rank Adaptation (LoRA) for parameter-efficient fine-tuning of our multimodal model, improving accuracy without significantly increasing compute or memory costs.

- **Evaluation & Monitoring**  
  Continuously measuring one-word answer accuracy and error patterns throughout training to ensure the model is effectively learning to correlate image features with textual queries.  

## Repository Structure

The project is organized as follows:
```
Multimodal-VQA/
├── Dataset/
│ ├── final_dataset/ # Folder containing all raw images
│ │ ├── img_0001.jpg
│ │ ├── img_0002.jpg
│ │ └── …
│ └── metadata/ # CSVs and metadata files
│ ├── image_data.csv # Original image + listing metadata
│ ├── image_data_with_vqa.csv # Generated Q&A pairs (intermediate)
│ └── merged_image_data_vqa.csv # Final merged dataset
├── Inference Script/
|   ├── requirements.txt
|   └── inference.py
├── Models/
|   └── blip-vqa.ipynb 
|   └── ....
├── .env # Environment file with API keys (!! ADD TO .gitignore !!)
├── requirements.txt # Python dependencies
├── make_vqa.py # Script to generate Q&A via API (with key rotation)
├── merge_csv.py # Script to merge multiple VQA CSVs into one
├── Mini-Project-Statement.pdf # Project statement and objectives
└── README.md # Project overview and usage instructions
```


### Key Components:

*   **`Dataset/`**: Contains all data used and generated by the project.
    *   **`Dataset/final_dataset/`**: Stores the raw image files (e.g., `img_0001.jpg`).
    *   **`Dataset/metadata/`**: Houses all CSV files and other metadata.
        *   `image_data.csv`: The initial dataset containing image identifiers and associated metadata.
        *   `image_data_with_vqa.csv`: An intermediate CSV file with generated VQA pairs from an initial run.
        *   `merged_image_data_vqa.csv`: The final, consolidated dataset combining all image metadata and VQA pairs.
*   **`Models/`**: Contains all models that were run.
*   **`Inference Script/`**: Contains the `inference.py` script and the `requirements.txt` file for it.
*   **`.env`**: Stores sensitive information like API keys. **Important:** This file should be listed in your `.gitignore` file to prevent committing secrets.
*   **`requirements.txt`**: Lists all Python packages required to run the project. Install with `pip install -r requirements.txt`.
*   **`make_vqa.py`**: Python script responsible for generating Visual Question Answering (VQA) pairs using Gemini API, likely incorporating logic for API key rotation to manage rate limits or usage quotas.
*   **`merge_csv.py`**: Utility script to combine multiple CSV files (e.g., from different VQA generation runs by the API . ) into a single, unified CSV.


## Prerequisites
- Python 3.9+
- Git  

## Dataset

We use the **Amazon–Berkeley Objects** (ABO) dataset for both images and metadata. You can download the raw data here:

https://amazon-berkeley-objects.s3.amazonaws.com/index.html#download

We used `abo-listings.tar` and `abo-images-small.tar` to curate our dataset.


## Installation
1. Clone the repository  
   ```bash
   git clone https://github.com/nathanmathewv/Multimodal-VQA.git
   cd Multimodal-VQA
   ```
2. Create and activate a virtual environment
    ```bash
    python -m venv venv
    # On Windows:
    venv\Scripts\activate
    # On macOS/Linux:
    source venv/bin/activate
    ```
3. Install Python dependencies
    ```bash
    pip install -r requirements.txt
    ```
4. Configuring API Keys 
    Create a  .env file and populate with keys eg:
    ```bash
    API_KEY_1=your_first_api_key
    API_KEY_2=your_second_api_key
    ```

## Running the Code

1. Create your Dataset folder from ABO
    After downloading the zip files from the [ABO website](https://amazon-berkeley-objects.s3.amazonaws.com/index.html#download), extract them and store it in a folder `Dataset/metadata`. 
    to run merge all the listings to a single json file `merged_listings.json`.
    ```bash
    run metadata_extraction.ipynb
    ```

    to match the metadata with the images and store a subset of images in `Dataset/final_dataset`
    ```bash
    run match_listing_image.ipynb
    ```

    We have around 20K images in our `Dataset/final_dataset` folder.
    
2. Create Visual Question Answer Pairs using Gemini API
    Choose a start index and an end index to generate question and answer pairs for your images stored in `Dataset/final_dataset` . Have enough API keys from gemini for this . 
    ```bash
    python make_vqa.py
    ```

3. Building VQA Models

    To train and benchmark our Visual Question Answering architectures, we use the “Images-with-VQAs” dataset which we uploaded to Kaggle after following the above steps. This collection provides paired images and concise, one-word answers—exactly. You can download it here:

    [Images-with-VQAs Dataset on Kaggle](https://www.kaggle.com/datasets/nathanmathew/images-with-vqas)

    ```bash
    run the jupyter notebooks interactively in Models/
    ```



## Running the Inference Code

To run the inference script using our fine-tuned BLIP model:

#### Requirements

Ensure you are in a Python 3.9 virtual environment and have installed the required dependencies:

```bash
cd 'Inference Script'
pip install -r requirements.txt
```

---

#### Image & CSV Path

* **Image Directory:**
   `IMAGE_DIR`

* **CSV File:**
  `CSV_PATH`

---

#### Run the Script

Use the following command to execute `inference.py`:

```bash
python3 inference.py \
  --image_dir IMAGE_DIR \
  --csv_path CSV_PATH
```

---

#### Output

The predictions will be saved to a file named:

```
results.csv
```

in the current directory.


## Report 
For a detailed explanation of the methodology, experiments, and results, see the full project report:
[Project_Report.pdf](https://github.com/nathanmathewv/Multimodal-VQA/blob/main/VR_Report.pdf)
