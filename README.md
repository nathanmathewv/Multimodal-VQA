# Multi Modal VQA

## Table of Contents
- [Introduction](#introduction)  
- [Repository Structure](#repository-structure)  
- [Prerequisites](#prerequisites)  
- [Installation](#installation)  
- [Running the Code](#running-the-inference-code)  
- [Results](#results)  
- [Report](#report)  

## Introduction

In this project, we tackle the task of **Visual Question Answering (VQA)**: given an image and a natural-language question about its contents, our system must produce a single, concise word as the answer. To train and evaluate this model, we use the **Amazon–Berkeley Objects** dataset—augmented with rich item metadata—to assemble a high-quality VQA corpus.

### Key steps in our approach include:

- **Dataset Curation**  
  Leveraging large language models to generate 3 diverse question–answer pairs per image based on both visual content and metadata, while enforcing constraints (e.g., no numerical answers).

- **Baseline Modeling**  
  Establishing a transformer baseline (e.g., BLIP, BLIP2) to validate end-to-end performance on the curated VQA pairs.

- **Fine-Tuning with LoRA**  
  Applying Low-Rank Adaptation (LoRA) for parameter-efficient fine-tuning of our multimodal model, improving accuracy without significantly increasing compute or memory costs.

- **Evaluation & Monitoring**  
  Continuously measuring one-word answer accuracy and error patterns throughout training to ensure the model is effectively learning to correlate image features with textual queries.  

## Repository Structure

The project is organized as follows:
```
Multimodal-VQA/
├── Dataset/
│ ├── final_dataset/ # Folder containing all raw images
│ │ ├── img_0001.jpg
│ │ ├── img_0002.jpg
│ │ └── …
│ └── metadata/ # CSVs and metadata files
│ ├── image_data.csv # Original image + listing metadata
│ ├── image_data_with_vqa.csv # Generated Q&A pairs (intermediate)
│ ├── image_data_with_vqa1.csv # Additional VQA runs
│ └── merged_image_data_vqa.csv # Final merged dataset
├── Inference Script/
|   ├── requirements.txt
|   └── inference.py
├── Models/
|   └── ..
├── .env # Environment file with API keys (!! ADD TO .gitignore !!)
├── requirements.txt # Python dependencies
├── make_vqa.py # Script to generate Q&A via API (with key rotation)
├── merge_csvs.py # Script to merge multiple VQA CSVs into one
├── count_yesno.py # Script to count yes/no questions in the dataset
├── Blip2_model.py # Script for zero‐shot inference & evaluation with BLIP-2
├── model_lora.py # Script to fine-tune the VQA model with LoRA
├── Mini-Project-Statement.pdf # Project statement and objectives
└── README.md # Project overview and usage instructions
```


### Key Components:

*   **`Dataset/`**: Contains all data used and generated by the project.
    *   **`Dataset/final_dataset/`**: Stores the raw image files (e.g., `img_0001.jpg`).
    *   **`Dataset/metadata/`**: Houses all CSV files and other metadata.
        *   `image_data.csv`: The initial dataset containing image identifiers and associated metadata.
        *   `image_data_with_vqa.csv`: An intermediate CSV file with generated VQA pairs from an initial run.
        *   `image_data_with_vqa1.csv`: CSV from additional VQA generation runs.
        *   `merged_image_data_vqa.csv`: The final, consolidated dataset combining all image metadata and VQA pairs.
*   **`.env`**: Stores sensitive information like API keys. **Important:** This file should be listed in your `.gitignore` file to prevent committing secrets.
*   **`requirements.txt`**: Lists all Python packages required to run the project. Install with `pip install -r requirements.txt`.
*   **`make_vqa.py`**: Python script responsible for generating Visual Question Answering (VQA) pairs using an external API, likely incorporating logic for API key rotation to manage rate limits or usage quotas.
*   **`merge_csvs.py`**: Utility script to combine multiple CSV files (e.g., from different VQA generation runs) into a single, unified CSV.
*   **`count_yesno.py`**: A script to analyze the generated VQA dataset and count the occurrences of "yes/no" type questions.
*   **`Blip2_model.py`**: Implements zero-shot inference and evaluation capabilities using the BLIP-2 model.
*   **`model_lora.py`**: Script for fine-tuning a VQA model using Low-Rank Adaptation (LoRA) techniques for efficient adaptation.
*   **`Mini-Project-Statement.pdf`**: The document outlining the project's goals, scope, and objectives.
*   **`README.md`**: This file, providing an overview of the project, setup instructions, and usage guidelines.


## Prerequisites
- Python 3.9+
- Git  

## Installation
1. Clone the repository  
   ```bash
   git clone https://github.com/yourusername/your-repo.git
   cd your-repo
   ```
2. Create and activate a virtual environment
    ```bash
    python -m venv venv
    # On Windows:
    venv\Scripts\activate
    # On macOS/Linux:
    source venv/bin/activate
    ```
3. Install Python dependencies
    ```bash
    pip install -r requirements.txt
    ```
4. Configuring API Keys 
    Create a  .env file and populate with keys eg:
    ```bash
    API_KEY_1=your_first_api_key
    API_KEY_2=your_second_api_key
    ```
5. Create your Dataset folder from ABO

## Running the Inference Code

To run the inference script using our fine-tuned BLIP model:

#### Requirements

Ensure you are in a Python 3.9 virtual environment and have installed the required dependencies:

```bash
pip install -r requirements.txt
```

---

#### Image & CSV Path

* **Image Directory:**
   `IMAGE_DIR`

* **CSV File:**
  `CSV_PATH`

---

#### Run the Script

Use the following command to execute `inference.py`:

```bash
python3 inference.py \
  --image_dir IMAGE_DIR \
  --csv_path CSV_PATH
```

---

#### Output

The predictions will be saved to a file named:

```
results.csv
```

in the current directory.


## Report 
For a detailed explanation of the methodology, experiments, and results, see the full project report:
[Project_Report.pdf](https://github.com/nathanmathewv/Multimodal-VQA/blob/main/VR_Report.pdf)
