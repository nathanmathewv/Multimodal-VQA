{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T13:24:52.833919Z",
     "iopub.status.busy": "2025-05-11T13:24:52.833546Z",
     "iopub.status.idle": "2025-05-11T13:26:09.033160Z",
     "shell.execute_reply": "2025-05-11T13:26:09.032333Z",
     "shell.execute_reply.started": "2025-05-11T13:24:52.833887Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers accelerate peft bitsandbytes datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-11T13:26:09.034709Z",
     "iopub.status.busy": "2025-05-11T13:26:09.034486Z",
     "iopub.status.idle": "2025-05-11T13:26:36.695509Z",
     "shell.execute_reply": "2025-05-11T13:26:36.694683Z",
     "shell.execute_reply.started": "2025-05-11T13:26:09.034688Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T13:26:36.696940Z",
     "iopub.status.busy": "2025-05-11T13:26:36.696376Z",
     "iopub.status.idle": "2025-05-11T13:26:36.867722Z",
     "shell.execute_reply": "2025-05-11T13:26:36.866979Z",
     "shell.execute_reply.started": "2025-05-11T13:26:36.696919Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "img = mpimg.imread('/kaggle/input/images-with-vqas/final_dataset/final_dataset/10496adb.jpg')\n",
    "plt.axis('off')\n",
    "imgplot = plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T13:26:36.869237Z",
     "iopub.status.busy": "2025-05-11T13:26:36.869019Z",
     "iopub.status.idle": "2025-05-11T13:26:39.082686Z",
     "shell.execute_reply": "2025-05-11T13:26:39.081945Z",
     "shell.execute_reply.started": "2025-05-11T13:26:36.869219Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "vqa_df = pd.read_csv('/kaggle/input/images-with-vqas/merged_image_data_vqa.csv')\n",
    "vqa_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T13:26:39.083837Z",
     "iopub.status.busy": "2025-05-11T13:26:39.083454Z",
     "iopub.status.idle": "2025-05-11T13:26:39.092478Z",
     "shell.execute_reply": "2025-05-11T13:26:39.091838Z",
     "shell.execute_reply.started": "2025-05-11T13:26:39.083817Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(vqa_df['vqa_response'].notna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T13:26:39.093343Z",
     "iopub.status.busy": "2025-05-11T13:26:39.093129Z",
     "iopub.status.idle": "2025-05-11T13:26:39.106701Z",
     "shell.execute_reply": "2025-05-11T13:26:39.105914Z",
     "shell.execute_reply.started": "2025-05-11T13:26:39.093329Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # detect and init the TPU\n",
    "# tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "\n",
    "# # instantiate a distribution strategy\n",
    "# tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "# tpu_strategy = tf.distribute.TPUStrategy(tpu)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T13:26:39.107844Z",
     "iopub.status.busy": "2025-05-11T13:26:39.107569Z",
     "iopub.status.idle": "2025-05-11T13:27:57.776117Z",
     "shell.execute_reply": "2025-05-11T13:27:57.775334Z",
     "shell.execute_reply.started": "2025-05-11T13:26:39.107814Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_id = \"llava-hf/bakLlava-v1-hf\"\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "# model = LlavaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "processor.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T13:29:58.310776Z",
     "iopub.status.busy": "2025-05-11T13:29:58.310437Z",
     "iopub.status.idle": "2025-05-11T13:30:08.599561Z",
     "shell.execute_reply": "2025-05-11T13:30:08.598780Z",
     "shell.execute_reply.started": "2025-05-11T13:29:58.310720Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "# For storing results\n",
    "predictions = []\n",
    "refs = []\n",
    "count = 0\n",
    "start_time = time.time()\n",
    "TIME_LIMIT = 42000\n",
    "\n",
    "\n",
    "with torch.no_grad():  \n",
    "    for idx, row in tqdm(vqa_df.iterrows(), total=len(vqa_df)):\n",
    "        elapsed_time = time.time()\n",
    "        if elapsed_time - start_time > TIME_LIMIT:\n",
    "            print(f\"\\nTime limit of {TIME_LIMIT} seconds exceeded. Aborting loop at index {idx}.\")\n",
    "            print(f'Number of successfully run images - {idx}')\n",
    "            break\n",
    "        image_path = os.path.join(\n",
    "            \"/kaggle/input/images-with-vqas/final_dataset/final_dataset\",\n",
    "            row[\"image_path\"].replace(\"Dataset/final_dataset/\", \"\")\n",
    "        )\n",
    "    \n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Image not found at {image_path}, skipping row {idx}\")\n",
    "            predictions.append(\"IMAGE_NOT_FOUND\")\n",
    "            refs.append(row[\"vqa_response\"].strip() if pd.notna(row[\"vqa_response\"]) else \"\")\n",
    "            # count += 1\n",
    "            # if count == 5:\n",
    "            #     break\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {image_path}: {e}, skipping row {idx}\")\n",
    "            predictions.append(\"IMAGE_ERROR\")\n",
    "            refs.append(row[\"vqa_response\"].strip() if pd.notna(row[\"vqa_response\"]) else \"\")\n",
    "            # count += 1\n",
    "            # if count == 5:\n",
    "            #     break\n",
    "            continue\n",
    "    \n",
    "        try:\n",
    "            vqa_pairs = ast.literal_eval(row[\"vqa_response\"])\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to parse vqa_response: {e}, skipping row {idx}\")\n",
    "            predictions.append(\"PARSE_ERROR\")\n",
    "            refs.append(row[\"vqa_response\"].strip() if pd.notna(row[\"vqa_response\"]) else \"\")\n",
    "            # count += 1\n",
    "            # if count == 5:\n",
    "            #     break\n",
    "            continue\n",
    "    \n",
    "        for q, gt_answer in vqa_pairs:\n",
    "            prompt = f\"<image>\\nBased on the image, answer the following question with a single word. Question: {q} Answer:\"\n",
    "    \n",
    "            # Process both text and image\n",
    "            inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "    \n",
    "            # Move tensors to the correct device and dtype\n",
    "            inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}  # Move to GPU first\n",
    "            # Selectively convert non-index tensors to float16\n",
    "            for k in inputs:\n",
    "                if k != \"input_ids\" and k != \"attention_mask\":  # Keep input_ids and attention_mask as long/int\n",
    "                    inputs[k] = inputs[k].to(torch.float16)\n",
    "    \n",
    "            # Generate output\n",
    "            generated_ids = model.generate(**inputs, max_new_tokens=10)\n",
    "            pred_full = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            # print(\"Raw Prediction\",pred_full)\n",
    "    \n",
    "            pred_full = pred_full[0].strip().split(':')\n",
    "            ans = pred_full[-1]\n",
    "    \n",
    "            # Debug raw output\n",
    "            print(f\"\\nIdx: {idx}\")\n",
    "            print(f\"Original Question: {q}\")\n",
    "            print(f\"Prompt Used: '{prompt}'\")\n",
    "            print(f\"Full Prediction: '{ans}'\")\n",
    "    \n",
    "            # Clean up the prediction to extract a single word\n",
    "            words = ans.split()\n",
    "            pred = words[0].rstrip('.,;:!?') if words else \"\"\n",
    "    \n",
    "            print(f\"Processed Prediction (Single Word Attempt): '{pred}'\")\n",
    "            print(f\"Ground Truth: {gt_answer}\")\n",
    "            print(f\"{'-'*50}\")\n",
    "    \n",
    "            predictions.append(pred)\n",
    "            refs.append(gt_answer.strip() if pd.notna(gt_answer) else \"\")\n",
    "            # count += 1\n",
    "            # if count == 5:  # Limiting to 5 for the example\n",
    "            #     print(\"\\nReached count limit (5). Breaking loop.\")\n",
    "            #     break\n",
    "    \n",
    "        # if count == 5:\n",
    "        #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-11T13:27:57.847917Z",
     "iopub.status.idle": "2025-05-11T13:27:57.848125Z",
     "shell.execute_reply": "2025-05-11T13:27:57.848035Z",
     "shell.execute_reply.started": "2025-05-11T13:27:57.848026Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install evaluate bert-score rouge-score rapidfuzz sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-11T13:27:57.848712Z",
     "iopub.status.idle": "2025-05-11T13:27:57.849005Z",
     "shell.execute_reply": "2025-05-11T13:27:57.848905Z",
     "shell.execute_reply.started": "2025-05-11T13:27:57.848891Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import evaluate\n",
    "from bert_score import BERTScorer\n",
    "from rouge_score import rouge_scorer\n",
    "from rapidfuzz.distance import Levenshtein\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Normalize case\n",
    "preds_l = [p.lower() for p in predictions]\n",
    "refs_l = [r.lower() for r in refs]\n",
    "\n",
    "# Compute exact-match binary metrics\n",
    "y_pred_bin = [int(p == r) for p, r in zip(preds_l, refs_l)]\n",
    "y_true_bin = [1] * len(refs)\n",
    "\n",
    "acc = accuracy_score(y_true_bin, y_pred_bin)\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "    y_true_bin, y_pred_bin, average=\"binary\", zero_division=0\n",
    ")\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Exact-match Accuracy: {acc:.3f}\")\n",
    "print(f\"Exact-match Precision: {prec:.3f}\")\n",
    "print(f\"Exact-match Recall:    {rec:.3f}\")\n",
    "print(f\"Exact-match F1:        {f1:.3f}\\n\")\n",
    "\n",
    "# Save predictions and ground truths to CSV\n",
    "pred_ref_df = pd.DataFrame({\n",
    "    \"Prediction\": predictions,\n",
    "    \"Ground_Truth\": refs\n",
    "})\n",
    "pred_ref_df.to_csv('prediction_output.csv', index=False)\n",
    "\n",
    "metrics_df = pd.DataFrame({\n",
    "\"Metric\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1\"],\n",
    "\"Value\": [acc, prec, rec, f1]\n",
    "})\n",
    "metrics_df.to_csv('exact_metrics.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-11T13:27:57.850090Z",
     "iopub.status.idle": "2025-05-11T13:27:57.850374Z",
     "shell.execute_reply": "2025-05-11T13:27:57.850231Z",
     "shell.execute_reply.started": "2025-05-11T13:27:57.850216Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Use second GPU\n",
    "device = \"cuda:1\" if torch.cuda.device_count() > 1 else \"cuda:0\"\n",
    "\n",
    "# Initialize result dictionary\n",
    "final_metrics = {}\n",
    "\n",
    "# --- ROUGE ---\n",
    "print(\"\\n--- ROUGE Scores ---\")\n",
    "rouge_eval_scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "rouge1_scores, rougeL_scores = [], []\n",
    "\n",
    "for pred, ref in zip(preds_l, refs_l):\n",
    "    if not pred or not ref:\n",
    "        rouge1_scores.append(0.0)\n",
    "        rougeL_scores.append(0.0)\n",
    "        continue\n",
    "    scores = rouge_eval_scorer.score(ref, pred)\n",
    "    rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "    rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "\n",
    "if rouge1_scores:\n",
    "    final_metrics[\"rouge1_f1\"] = np.mean(rouge1_scores)\n",
    "    final_metrics[\"rougeL_f1\"] = np.mean(rougeL_scores)\n",
    "    print(f\"Average ROUGE-1 F1: {final_metrics['rouge1_f1']:.3f}\")\n",
    "    print(f\"Average ROUGE-L F1: {final_metrics['rougeL_f1']:.3f}\")\n",
    "else:\n",
    "    print(\"No ROUGE scores to compute.\")\n",
    "\n",
    "# --- BERTScore ---\n",
    "print(\"\\n--- BERTScore ---\")\n",
    "try:\n",
    "    bert_eval_scorer = BERTScorer(lang=\"en\", rescale_with_baseline=True, device=device)\n",
    "    filtered_preds_l = [p for p, r in zip(preds_l, refs_l) if p and r]\n",
    "    filtered_refs_l = [r for p, r in zip(preds_l, refs_l) if p and r]\n",
    "\n",
    "    if filtered_preds_l and filtered_refs_l:\n",
    "        P, R, F1 = bert_eval_scorer.score(filtered_preds_l, filtered_refs_l)\n",
    "        final_metrics[\"bertscore_precision\"] = P.mean().item()\n",
    "        final_metrics[\"bertscore_recall\"] = R.mean().item()\n",
    "        final_metrics[\"bertscore_f1\"] = F1.mean().item()\n",
    "        print(f\"Average BERTScore Precision: {final_metrics['bertscore_precision']:.3f}\")\n",
    "        print(f\"Average BERTScore Recall:    {final_metrics['bertscore_recall']:.3f}\")\n",
    "        print(f\"Average BERTScore F1:        {final_metrics['bertscore_f1']:.3f}\")\n",
    "    else:\n",
    "        print(\"Not enough valid pairs for BERTScore.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not compute BERTScore: {e}\")\n",
    "\n",
    "# --- Levenshtein Normalized Similarity ---\n",
    "print(\"\\n--- Levenshtein Normalized Similarity ---\")\n",
    "lev_similarities = []\n",
    "for pred, ref in zip(preds_l, refs_l):\n",
    "    if not pred and not ref:\n",
    "        similarity = 1.0\n",
    "    elif not pred or not ref:\n",
    "        similarity = 0.0\n",
    "    else:\n",
    "        similarity = Levenshtein.normalized_similarity(pred, ref)\n",
    "    lev_similarities.append(similarity)\n",
    "\n",
    "if lev_similarities:\n",
    "    final_metrics[\"levenshtein_sim\"] = np.mean(lev_similarities)\n",
    "    print(f\"Average Levenshtein Normalized Similarity: {final_metrics['levenshtein_sim']:.3f}\")\n",
    "else:\n",
    "    print(\"No Levenshtein similarities to compute.\")\n",
    "\n",
    "# --- Sentence-BERT Cosine Similarity ---\n",
    "print(\"\\n--- Sentence-BERT Cosine Similarity ---\")\n",
    "try:\n",
    "    sbert_model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "    valid_pairs = [(p, r) for p, r in zip(preds_l, refs_l) if p and r]\n",
    "    if valid_pairs:\n",
    "        sbert_preds, sbert_refs = zip(*valid_pairs)\n",
    "        embeddings_preds = sbert_model.encode(list(sbert_preds), convert_to_tensor=True)\n",
    "        embeddings_refs = sbert_model.encode(list(sbert_refs), convert_to_tensor=True)\n",
    "        cosine_scores = util.cos_sim(embeddings_preds, embeddings_refs)\n",
    "        pairwise_sim = [cosine_scores[i, i].item() for i in range(len(valid_pairs))]\n",
    "        final_metrics[\"sbert_cosine_sim\"] = np.mean(pairwise_sim)\n",
    "        print(f\"Average Sentence-BERT Cosine Similarity: {final_metrics['sbert_cosine_sim']:.3f}\")\n",
    "    else:\n",
    "        print(\"Not enough valid pairs for Sentence-BERT similarity.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not compute Sentence-BERT similarity: {e}\")\n",
    "\n",
    "# Save final metrics to CSV\n",
    "df_metrics = pd.DataFrame([final_metrics])\n",
    "df_metrics.to_csv(\"alternate_metrics.csv\", index=False)\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n--- Final Metrics ---\")\n",
    "for k, v in final_metrics.items():\n",
    "    print(f\"{k}: {v:.3f}\")\n",
    "\n",
    "print(\"\\nFinal metrics saved to 'metrics_output.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7377164,
     "sourceId": 11762420,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
