{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11762420,"sourceType":"datasetVersion","datasetId":7377164}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers accelerate peft bitsandbytes datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T13:24:52.833546Z","iopub.execute_input":"2025-05-11T13:24:52.833919Z","iopub.status.idle":"2025-05-11T13:26:09.033160Z","shell.execute_reply.started":"2025-05-11T13:24:52.833887Z","shell.execute_reply":"2025-05-11T13:26:09.032333Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoProcessor, LlavaForConditionalGeneration\nfrom tqdm import tqdm\nfrom datasets import load_dataset, Dataset\nimport pandas as pd\nimport torch\nfrom PIL import Image\nimport os\nimport ast","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T13:26:09.034486Z","iopub.execute_input":"2025-05-11T13:26:09.034709Z","iopub.status.idle":"2025-05-11T13:26:36.695509Z","shell.execute_reply.started":"2025-05-11T13:26:09.034688Z","shell.execute_reply":"2025-05-11T13:26:36.694683Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimg = mpimg.imread('/kaggle/input/images-with-vqas/final_dataset/final_dataset/10496adb.jpg')\nplt.axis('off')\nimgplot = plt.imshow(img)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T13:26:36.696376Z","iopub.execute_input":"2025-05-11T13:26:36.696940Z","iopub.status.idle":"2025-05-11T13:26:36.867722Z","shell.execute_reply.started":"2025-05-11T13:26:36.696919Z","shell.execute_reply":"2025-05-11T13:26:36.866979Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vqa_df = pd.read_csv('/kaggle/input/images-with-vqas/merged_image_data_vqa.csv')\nvqa_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T13:26:36.869019Z","iopub.execute_input":"2025-05-11T13:26:36.869237Z","iopub.status.idle":"2025-05-11T13:26:39.082686Z","shell.execute_reply.started":"2025-05-11T13:26:36.869219Z","shell.execute_reply":"2025-05-11T13:26:39.081945Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(vqa_df['vqa_response'].notna())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T13:26:39.083454Z","iopub.execute_input":"2025-05-11T13:26:39.083837Z","iopub.status.idle":"2025-05-11T13:26:39.092478Z","shell.execute_reply.started":"2025-05-11T13:26:39.083817Z","shell.execute_reply":"2025-05-11T13:26:39.091838Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # detect and init the TPU\n# tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n\n# # instantiate a distribution strategy\n# tf.tpu.experimental.initialize_tpu_system(tpu)\n# tpu_strategy = tf.distribute.TPUStrategy(tpu)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T13:26:39.093129Z","iopub.execute_input":"2025-05-11T13:26:39.093343Z","iopub.status.idle":"2025-05-11T13:26:39.106701Z","shell.execute_reply.started":"2025-05-11T13:26:39.093329Z","shell.execute_reply":"2025-05-11T13:26:39.105914Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_id = \"llava-hf/bakLlava-v1-hf\"\nprocessor = AutoProcessor.from_pretrained(model_id)\nmodel = LlavaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.float16)\nmodel.eval().to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T13:26:39.107569Z","iopub.execute_input":"2025-05-11T13:26:39.107844Z","iopub.status.idle":"2025-05-11T13:27:57.776117Z","shell.execute_reply.started":"2025-05-11T13:26:39.107814Z","shell.execute_reply":"2025-05-11T13:27:57.775334Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# VERIFY MODEL TYPE IMMEDIATELY AFTER LOADING\nprint(f\"Type of 'model' after loading: {type(model)}\")\nassert isinstance(model, LlavaForConditionalGeneration), \"Model is not a LlavaForConditionalGeneration instance!\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T13:27:57.776972Z","iopub.execute_input":"2025-05-11T13:27:57.777301Z","iopub.status.idle":"2025-05-11T13:27:57.781479Z","shell.execute_reply.started":"2025-05-11T13:27:57.777270Z","shell.execute_reply":"2025-05-11T13:27:57.780964Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\n# For storing results\npredictions = []\nrefs = []\ncount = 0\nstart_time = time.time()\nTIME_LIMIT = 42000\n\n\nwith torch.no_grad():  \n    for idx, row in tqdm(vqa_df.iterrows(), total=len(vqa_df)):\n        elapsed_time = time.time()\n        if elapsed_time - start_time > TIME_LIMIT:\n            print(f\"\\nTime limit of {TIME_LIMIT} seconds exceeded. Aborting loop at index {idx}.\")\n            print(f'Number of successfully run images - {idx}')\n            break\n        image_path = os.path.join(\n            \"/kaggle/input/images-with-vqas/final_dataset/final_dataset\",\n            row[\"image_path\"].replace(\"Dataset/final_dataset/\", \"\")\n        )\n    \n        try:\n            image = Image.open(image_path).convert(\"RGB\")\n        except FileNotFoundError:\n            print(f\"Warning: Image not found at {image_path}, skipping row {idx}\")\n            predictions.append(\"IMAGE_NOT_FOUND\")\n            refs.append(row[\"vqa_response\"].strip() if pd.notna(row[\"vqa_response\"]) else \"\")\n            # count += 1\n            # if count == 5:\n            #     break\n            continue\n        except Exception as e:\n            print(f\"Error loading image {image_path}: {e}, skipping row {idx}\")\n            predictions.append(\"IMAGE_ERROR\")\n            refs.append(row[\"vqa_response\"].strip() if pd.notna(row[\"vqa_response\"]) else \"\")\n            # count += 1\n            # if count == 5:\n            #     break\n            continue\n    \n        try:\n            vqa_pairs = ast.literal_eval(row[\"vqa_response\"])\n        except Exception as e:\n            print(f\"Failed to parse vqa_response: {e}, skipping row {idx}\")\n            predictions.append(\"PARSE_ERROR\")\n            refs.append(row[\"vqa_response\"].strip() if pd.notna(row[\"vqa_response\"]) else \"\")\n            # count += 1\n            # if count == 5:\n            #     break\n            continue\n    \n        for q, gt_answer in vqa_pairs:\n            prompt = f\"<image>\\nBased on the image, answer the following question with a single word. Question: {q} Answer:\"\n    \n            # Process both text and image\n            inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n    \n            # Move tensors to the correct device and dtype\n            inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}  # Move to GPU first\n            # Selectively convert non-index tensors to float16\n            for k in inputs:\n                if k != \"input_ids\" and k != \"attention_mask\":  # Keep input_ids and attention_mask as long/int\n                    inputs[k] = inputs[k].to(torch.float16)\n    \n            # Generate output\n            generated_ids = model.generate(**inputs, max_new_tokens=10)\n            pred_full = processor.batch_decode(generated_ids, skip_special_tokens=True)\n            # print(\"Raw Prediction\",pred_full)\n    \n            pred_full = pred_full[0].strip().split(':')\n            ans = pred_full[-1]\n    \n            # Debug raw output\n            print(f\"\\nIdx: {idx}\")\n            print(f\"Original Question: {q}\")\n            print(f\"Prompt Used: '{prompt}'\")\n            print(f\"Full Prediction: '{ans}'\")\n    \n            # Clean up the prediction to extract a single word\n            words = ans.split()\n            pred = words[0].rstrip('.,;:!?') if words else \"\"\n    \n            print(f\"Processed Prediction (Single Word Attempt): '{pred}'\")\n            print(f\"Ground Truth: {gt_answer}\")\n            print(f\"{'-'*50}\")\n    \n            predictions.append(pred)\n            refs.append(gt_answer.strip() if pd.notna(gt_answer) else \"\")\n            # count += 1\n            # if count == 5:  # Limiting to 5 for the example\n            #     print(\"\\nReached count limit (5). Breaking loop.\")\n            #     break\n    \n        # if count == 5:\n        #     break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T13:29:58.310437Z","iopub.execute_input":"2025-05-11T13:29:58.310776Z","iopub.status.idle":"2025-05-11T13:30:08.599561Z","shell.execute_reply.started":"2025-05-11T13:29:58.310720Z","shell.execute_reply":"2025-05-11T13:30:08.598780Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install evaluate bert-score rouge-score rapidfuzz sentence-transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T13:27:57.847917Z","iopub.status.idle":"2025-05-11T13:27:57.848125Z","shell.execute_reply.started":"2025-05-11T13:27:57.848026Z","shell.execute_reply":"2025-05-11T13:27:57.848035Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_recall_fscore_support\nimport evaluate\nfrom bert_score import BERTScorer\nfrom rouge_score import rouge_scorer\nfrom rapidfuzz.distance import Levenshtein\nfrom sentence_transformers import SentenceTransformer, util\nimport numpy as np\nimport pandas as pd\n\n# Normalize case\npreds_l = [p.lower() for p in predictions]\nrefs_l = [r.lower() for r in refs]\n\n# Compute exact-match binary metrics\ny_pred_bin = [int(p == r) for p, r in zip(preds_l, refs_l)]\ny_true_bin = [1] * len(refs)\n\nacc = accuracy_score(y_true_bin, y_pred_bin)\nprec, rec, f1, _ = precision_recall_fscore_support(\n    y_true_bin, y_pred_bin, average=\"binary\", zero_division=0\n)\n\n# Print metrics\nprint(f\"Exact-match Accuracy: {acc:.3f}\")\nprint(f\"Exact-match Precision: {prec:.3f}\")\nprint(f\"Exact-match Recall:    {rec:.3f}\")\nprint(f\"Exact-match F1:        {f1:.3f}\\n\")\n\n# Save predictions and ground truths to CSV\npred_ref_df = pd.DataFrame({\n    \"Prediction\": predictions,\n    \"Ground_Truth\": refs\n})\npred_ref_df.to_csv('prediction_output.csv', index=False)\n\nmetrics_df = pd.DataFrame({\n\"Metric\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1\"],\n\"Value\": [acc, prec, rec, f1]\n})\nmetrics_df.to_csv('exact_metrics.csv',index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T13:27:57.848712Z","iopub.status.idle":"2025-05-11T13:27:57.849005Z","shell.execute_reply.started":"2025-05-11T13:27:57.848891Z","shell.execute_reply":"2025-05-11T13:27:57.848905Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Use second GPU\ndevice = \"cuda:1\" if torch.cuda.device_count() > 1 else \"cuda:0\"\n\n# Initialize result dictionary\nfinal_metrics = {}\n\n# --- ROUGE ---\nprint(\"\\n--- ROUGE Scores ---\")\nrouge_eval_scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\nrouge1_scores, rougeL_scores = [], []\n\nfor pred, ref in zip(preds_l, refs_l):\n    if not pred or not ref:\n        rouge1_scores.append(0.0)\n        rougeL_scores.append(0.0)\n        continue\n    scores = rouge_eval_scorer.score(ref, pred)\n    rouge1_scores.append(scores['rouge1'].fmeasure)\n    rougeL_scores.append(scores['rougeL'].fmeasure)\n\nif rouge1_scores:\n    final_metrics[\"rouge1_f1\"] = np.mean(rouge1_scores)\n    final_metrics[\"rougeL_f1\"] = np.mean(rougeL_scores)\n    print(f\"Average ROUGE-1 F1: {final_metrics['rouge1_f1']:.3f}\")\n    print(f\"Average ROUGE-L F1: {final_metrics['rougeL_f1']:.3f}\")\nelse:\n    print(\"No ROUGE scores to compute.\")\n\n# --- BERTScore ---\nprint(\"\\n--- BERTScore ---\")\ntry:\n    bert_eval_scorer = BERTScorer(lang=\"en\", rescale_with_baseline=True, device=device)\n    filtered_preds_l = [p for p, r in zip(preds_l, refs_l) if p and r]\n    filtered_refs_l = [r for p, r in zip(preds_l, refs_l) if p and r]\n\n    if filtered_preds_l and filtered_refs_l:\n        P, R, F1 = bert_eval_scorer.score(filtered_preds_l, filtered_refs_l)\n        final_metrics[\"bertscore_precision\"] = P.mean().item()\n        final_metrics[\"bertscore_recall\"] = R.mean().item()\n        final_metrics[\"bertscore_f1\"] = F1.mean().item()\n        print(f\"Average BERTScore Precision: {final_metrics['bertscore_precision']:.3f}\")\n        print(f\"Average BERTScore Recall:    {final_metrics['bertscore_recall']:.3f}\")\n        print(f\"Average BERTScore F1:        {final_metrics['bertscore_f1']:.3f}\")\n    else:\n        print(\"Not enough valid pairs for BERTScore.\")\nexcept Exception as e:\n    print(f\"Could not compute BERTScore: {e}\")\n\n# --- Levenshtein Normalized Similarity ---\nprint(\"\\n--- Levenshtein Normalized Similarity ---\")\nlev_similarities = []\nfor pred, ref in zip(preds_l, refs_l):\n    if not pred and not ref:\n        similarity = 1.0\n    elif not pred or not ref:\n        similarity = 0.0\n    else:\n        similarity = Levenshtein.normalized_similarity(pred, ref)\n    lev_similarities.append(similarity)\n\nif lev_similarities:\n    final_metrics[\"levenshtein_sim\"] = np.mean(lev_similarities)\n    print(f\"Average Levenshtein Normalized Similarity: {final_metrics['levenshtein_sim']:.3f}\")\nelse:\n    print(\"No Levenshtein similarities to compute.\")\n\n# --- Sentence-BERT Cosine Similarity ---\nprint(\"\\n--- Sentence-BERT Cosine Similarity ---\")\ntry:\n    sbert_model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n    valid_pairs = [(p, r) for p, r in zip(preds_l, refs_l) if p and r]\n    if valid_pairs:\n        sbert_preds, sbert_refs = zip(*valid_pairs)\n        embeddings_preds = sbert_model.encode(list(sbert_preds), convert_to_tensor=True)\n        embeddings_refs = sbert_model.encode(list(sbert_refs), convert_to_tensor=True)\n        cosine_scores = util.cos_sim(embeddings_preds, embeddings_refs)\n        pairwise_sim = [cosine_scores[i, i].item() for i in range(len(valid_pairs))]\n        final_metrics[\"sbert_cosine_sim\"] = np.mean(pairwise_sim)\n        print(f\"Average Sentence-BERT Cosine Similarity: {final_metrics['sbert_cosine_sim']:.3f}\")\n    else:\n        print(\"Not enough valid pairs for Sentence-BERT similarity.\")\nexcept Exception as e:\n    print(f\"Could not compute Sentence-BERT similarity: {e}\")\n\n# Save final metrics to CSV\ndf_metrics = pd.DataFrame([final_metrics])\ndf_metrics.to_csv(\"alternate_metrics.csv\", index=False)\n\n# Print summary\nprint(\"\\n--- Final Metrics ---\")\nfor k, v in final_metrics.items():\n    print(f\"{k}: {v:.3f}\")\n\nprint(\"\\nFinal metrics saved to 'metrics_output.csv'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T13:27:57.850090Z","iopub.status.idle":"2025-05-11T13:27:57.850374Z","shell.execute_reply.started":"2025-05-11T13:27:57.850216Z","shell.execute_reply":"2025-05-11T13:27:57.850231Z"}},"outputs":[],"execution_count":null}]}