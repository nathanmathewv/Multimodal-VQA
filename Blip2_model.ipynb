{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33001178",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468c90e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Paths to the two CSV files\n",
    "csv_file1 = \"Dataset/metadata/image_data_with_vqa.csv\"\n",
    "csv_file2 = \"Dataset/metadata/image_data_with_vqa1.csv\"\n",
    "\n",
    "# Load the CSV files into DataFrames\n",
    "df1 = pd.read_csv(csv_file1)\n",
    "df2 = pd.read_csv(csv_file2)\n",
    "\n",
    "# List to store tuples of differing image paths and a counter\n",
    "differences = []\n",
    "count = 0\n",
    "\n",
    "# Check that both dataframes have the same number of rows\n",
    "if len(df1) != len(df2):\n",
    "    print(\"Warning: The two CSV files have different number of rows.\")\n",
    "\n",
    "# Loop through each row assuming the rows correspond to each other\n",
    "for idx in range(min(len(df1), len(df2))):\n",
    "    path1 = df1.iloc[idx]['image_path']\n",
    "    path2 = df2.iloc[idx]['image_path']\n",
    "    if path1 != path2:\n",
    "        count += 1\n",
    "        differences.append((path1, path2))\n",
    "\n",
    "print(f\"Count of differing image paths: {count}\")\n",
    "print(\"First 5 differences:\")\n",
    "print(differences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e58812",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "img = mpimg.imread('Dataset/final_dataset/10496adb.jpg')\n",
    "plt.axis('off')\n",
    "imgplot = plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c26b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images in Dataset/final_dataset: 19795\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "folder_path = \"Dataset/final_dataset\"\n",
    "image_extensions = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\"}\n",
    "all_files = os.listdir(folder_path)\n",
    "image_files = [f for f in all_files if os.path.splitext(f)[1].lower() in image_extensions]\n",
    "print(f\"Total images in {folder_path}: {len(image_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae6ac057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 0 missing images\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    \"Dataset/metadata/image_data_with_vqa1.csv\"\n",
    ")\n",
    "\n",
    "# df[\"vqa_response\"] = df[\"vqa_response\"].apply(ast.literal_eval)\n",
    "# df = df.explode(\"vqa_response\").reset_index(drop=True)\n",
    "# df[[\"question\",\"answer\"]] = pd.DataFrame(df[\"vqa_response\"].tolist(), index=df.index)\n",
    "# drop missing images\n",
    "exists = df[\"image_path\"].apply(os.path.exists)\n",
    "print(f\"Skipping {(~exists).sum()} missing images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb79c72",
   "metadata": {},
   "source": [
    "### Model Pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029a9dfd",
   "metadata": {},
   "source": [
    "#### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f79360",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 1. Install dependencies\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "! pip install -q transformers torch evaluate scikit-learn pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d8af46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 2. Imports & Environment\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "import os\n",
    "import ast\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split  \n",
    "import evaluate                                      \n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833737dd",
   "metadata": {},
   "source": [
    "#### Pre-processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0c603f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 3. Load & Preprocess Dataset\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "\n",
    "CSV_PATH = \"/Datset/metadata/image_data_with_vqa.csv\"\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# df[\"image_path\"] = df[\"image_path\"].str.replace(\n",
    "#     r\"^Dataset/final_dataset/\",\n",
    "#     \"/kaggle/input/vqa-multimodal-sarvesh-nathan-divyam/final_dataset/final_dataset/\",\n",
    "#     regex=True\n",
    "# )\n",
    "\n",
    "# filter out rows with missing vqa_response\n",
    "df = df[df[\"vqa_response\"].notna()]\n",
    "df[\"vqa_response\"] = df[\"vqa_response\"].apply(ast.literal_eval)\n",
    "df = df.explode(\"vqa_response\").reset_index(drop=True)\n",
    "df[[\"question\", \"answer\"]] = pd.DataFrame(df[\"vqa_response\"].tolist(), index=df.index)\n",
    "\n",
    "exists = df[\"image_path\"].apply(os.path.exists)\n",
    "print(f\"⚠️  Skipping {(~exists).sum()} rows with missing images\")\n",
    "df = df[exists]\n",
    "df = df[[\"image_path\", \"question\", \"answer\"]].reset_index(drop=True)\n",
    "print(f\"Total examples: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a15a70",
   "metadata": {},
   "source": [
    "#### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b9fa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 4. Load BLIP-2 + FLAN-T5-XL (zero-shot) — unchanged\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")\n",
    "model     = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip2-flan-t5-xl\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.eval().to(device)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 5. Inference over the entire dataset\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "preds, refs = [], []\n",
    "with torch.no_grad():\n",
    "    for idx, row in df.iterrows():\n",
    "        img = Image.open(row.image_path).convert(\"RGB\")\n",
    "        inputs = processor(images=img, text=row.question, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        generated_ids = model.generate(**inputs)\n",
    "        pred = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "        print('idx: ',idx)\n",
    "        print('prediction: ', pred)\n",
    "        preds.append(pred)\n",
    "        refs.append(row.answer.strip())\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 6. Compute Accuracy on full set\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "acc = evaluate.load(\"accuracy\")\n",
    "results = acc.compute(references=refs, predictions=preds)\n",
    "print(f\"\\n🔍 Zero-Shot Accuracy on entire dataset = {results['accuracy']:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
