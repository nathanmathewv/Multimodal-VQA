{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11762420,"sourceType":"datasetVersion","datasetId":7377164}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n        # print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-10T20:27:08.838916Z","iopub.execute_input":"2025-05-10T20:27:08.839094Z","iopub.status.idle":"2025-05-10T20:27:10.743899Z","shell.execute_reply.started":"2025-05-10T20:27:08.839077Z","shell.execute_reply":"2025-05-10T20:27:10.743144Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install -q transformers torch evaluate scikit-learn pillow","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T20:27:10.745327Z","iopub.execute_input":"2025-05-10T20:27:10.745652Z","iopub.status.idle":"2025-05-10T20:28:26.495002Z","shell.execute_reply.started":"2025-05-10T20:27:10.745632Z","shell.execute_reply":"2025-05-10T20:28:26.494270Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport ast\nimport pandas as pd\nfrom PIL import Image\nimport torch\nfrom sklearn.model_selection import train_test_split  # scikit-learn split :contentReference[oaicite:2]{index=2}\nimport evaluate                                        # metrics :contentReference[oaicite:3]{index=3}\nfrom transformers import Blip2Processor, Blip2ForConditionalGeneration\n\nCSV_PATH = \"/kaggle/input/images-with-vqas/merged_image_data_vqa.csv\"\ndf = pd.read_csv(CSV_PATH)\n\ndf[\"image_path\"] = df[\"image_path\"].str.replace(\n    r\"^Dataset/final_dataset/\",\n    \"/kaggle/input/images-with-vqas/final_dataset/final_dataset/\",\n    regex=True\n)\n\ndf[\"vqa_response\"] = df[\"vqa_response\"].apply(ast.literal_eval)\ndf = df.explode(\"vqa_response\").reset_index(drop=True)\ndf[[\"question\", \"answer\"]] = pd.DataFrame(df[\"vqa_response\"].tolist(), index=df.index)\n\nexists = df[\"image_path\"].apply(os.path.exists)\nprint(f\"Skipping {(~exists).sum()} rows with missing images\")\ndf = df[exists]\n\ndf = df[[\"image_path\", \"question\", \"answer\"]].reset_index(drop=True)\nprint(f\"Total examples: {len(df)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T20:28:26.496100Z","iopub.execute_input":"2025-05-10T20:28:26.496401Z","iopub.status.idle":"2025-05-10T20:29:30.739567Z","shell.execute_reply.started":"2025-05-10T20:28:26.496371Z","shell.execute_reply":"2025-05-10T20:29:30.738640Z"}},"outputs":[{"name":"stderr","text":"2025-05-10 20:28:36.989176: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746908917.193221      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746908917.250811      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Skipping 0 rows with missing images\nTotal examples: 58124\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"df.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T20:29:30.740581Z","iopub.execute_input":"2025-05-10T20:29:30.740859Z","iopub.status.idle":"2025-05-10T20:29:30.758988Z","shell.execute_reply.started":"2025-05-10T20:29:30.740835Z","shell.execute_reply":"2025-05-10T20:29:30.758456Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                          image_path  \\\n0  /kaggle/input/images-with-vqas/final_dataset/f...   \n1  /kaggle/input/images-with-vqas/final_dataset/f...   \n2  /kaggle/input/images-with-vqas/final_dataset/f...   \n3  /kaggle/input/images-with-vqas/final_dataset/f...   \n4  /kaggle/input/images-with-vqas/final_dataset/f...   \n\n                                         question     answer  \n0             What is the main color of the shoe?      Brown  \n1                What is the pattern on the shoe?  Snakeskin  \n2                    Does the shoe have a tassel?        Yes  \n3               What color are the drawer slides?      White  \n4  What appears to be the material of the slides?      Metal  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_path</th>\n      <th>question</th>\n      <th>answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/images-with-vqas/final_dataset/f...</td>\n      <td>What is the main color of the shoe?</td>\n      <td>Brown</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/images-with-vqas/final_dataset/f...</td>\n      <td>What is the pattern on the shoe?</td>\n      <td>Snakeskin</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/images-with-vqas/final_dataset/f...</td>\n      <td>Does the shoe have a tassel?</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/images-with-vqas/final_dataset/f...</td>\n      <td>What color are the drawer slides?</td>\n      <td>White</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/images-with-vqas/final_dataset/f...</td>\n      <td>What appears to be the material of the slides?</td>\n      <td>Metal</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"from transformers import Blip2Processor, Blip2ForConditionalGeneration\nimport torch\nfrom PIL import Image\n\nprocessor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")\nmodel     = Blip2ForConditionalGeneration.from_pretrained(\n    \"Salesforce/blip2-flan-t5-xl\",\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.eval().to(device)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T20:29:30.759679Z","iopub.execute_input":"2025-05-10T20:29:30.760366Z","iopub.status.idle":"2025-05-10T20:32:15.734932Z","shell.execute_reply.started":"2025-05-10T20:29:30.760341Z","shell.execute_reply":"2025-05-10T20:32:15.734056Z"}},"outputs":[{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/432 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c330b7fc17b4b54b0fea5fdde5c6b71"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/21.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1cfaff1612a4375bfc8828c0b403b8f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fddb21e12610493e9f5d0339acb6e10a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42471d9e79ef4935b25e77dea68e5e2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/23.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00511311384047b7a1c47d6b727db110"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f92cc1e1c064c6ca10afe2ec7607760"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"processor_config.json:   0%|          | 0.00/68.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a33ed5b25e804e6c96ca3483c3d89ca5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/2.22k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72358f13f2c040ec9e770cf87c42261a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/128k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f2f3284e2de4a25874630dae9840647"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea1d6868a2a74179b622fa1264ac2c8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/5.81G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17656ed214d6430787204cfab4c5b2ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4efc26c2e444cf9ad9ea6c9994e9252"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f070b039d3db4dc797dde91d229bafbe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/168 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b243351a66640efbc5b8d5807968a70"}},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"Blip2ForConditionalGeneration(\n  (vision_model): Blip2VisionModel(\n    (embeddings): Blip2VisionEmbeddings(\n      (patch_embedding): Conv2d(3, 1408, kernel_size=(14, 14), stride=(14, 14))\n    )\n    (encoder): Blip2Encoder(\n      (layers): ModuleList(\n        (0-38): 39 x Blip2EncoderLayer(\n          (self_attn): Blip2Attention(\n            (dropout): Dropout(p=0.0, inplace=False)\n            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n          )\n          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n          (mlp): Blip2MLP(\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n          )\n          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n        )\n      )\n    )\n    (post_layernorm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n  )\n  (qformer): Blip2QFormerModel(\n    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (encoder): Blip2QFormerEncoder(\n      (layer): ModuleList(\n        (0): Blip2QFormerLayer(\n          (attention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (crossattention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=1408, out_features=768, bias=True)\n              (value): Linear(in_features=1408, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate_query): Blip2QFormerIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output_query): Blip2QFormerOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): Blip2QFormerLayer(\n          (attention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate_query): Blip2QFormerIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output_query): Blip2QFormerOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): Blip2QFormerLayer(\n          (attention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (crossattention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=1408, out_features=768, bias=True)\n              (value): Linear(in_features=1408, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate_query): Blip2QFormerIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output_query): Blip2QFormerOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): Blip2QFormerLayer(\n          (attention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate_query): Blip2QFormerIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output_query): Blip2QFormerOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): Blip2QFormerLayer(\n          (attention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (crossattention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=1408, out_features=768, bias=True)\n              (value): Linear(in_features=1408, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate_query): Blip2QFormerIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output_query): Blip2QFormerOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): Blip2QFormerLayer(\n          (attention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate_query): Blip2QFormerIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output_query): Blip2QFormerOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): Blip2QFormerLayer(\n          (attention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (crossattention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=1408, out_features=768, bias=True)\n              (value): Linear(in_features=1408, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate_query): Blip2QFormerIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output_query): Blip2QFormerOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): Blip2QFormerLayer(\n          (attention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate_query): Blip2QFormerIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output_query): Blip2QFormerOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): Blip2QFormerLayer(\n          (attention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (crossattention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=1408, out_features=768, bias=True)\n              (value): Linear(in_features=1408, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate_query): Blip2QFormerIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output_query): Blip2QFormerOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): Blip2QFormerLayer(\n          (attention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate_query): Blip2QFormerIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output_query): Blip2QFormerOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): Blip2QFormerLayer(\n          (attention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (crossattention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=1408, out_features=768, bias=True)\n              (value): Linear(in_features=1408, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate_query): Blip2QFormerIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output_query): Blip2QFormerOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): Blip2QFormerLayer(\n          (attention): Blip2QFormerAttention(\n            (attention): Blip2QFormerMultiHeadAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): Blip2QFormerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate_query): Blip2QFormerIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output_query): Blip2QFormerOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (language_projection): Linear(in_features=768, out_features=2048, bias=True)\n  (language_model): T5ForConditionalGeneration(\n    (shared): Embedding(32128, 2048)\n    (encoder): T5Stack(\n      (embed_tokens): Embedding(32128, 2048)\n      (block): ModuleList(\n        (0): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=2048, out_features=2048, bias=False)\n                (k): Linear(in_features=2048, out_features=2048, bias=False)\n                (v): Linear(in_features=2048, out_features=2048, bias=False)\n                (o): Linear(in_features=2048, out_features=2048, bias=False)\n                (relative_attention_bias): Embedding(32, 32)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseGatedActDense(\n                (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n                (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n                (wo): Linear(in_features=5120, out_features=2048, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): GELUActivation()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (1-23): 23 x T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=2048, out_features=2048, bias=False)\n                (k): Linear(in_features=2048, out_features=2048, bias=False)\n                (v): Linear(in_features=2048, out_features=2048, bias=False)\n                (o): Linear(in_features=2048, out_features=2048, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseGatedActDense(\n                (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n                (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n                (wo): Linear(in_features=5120, out_features=2048, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): GELUActivation()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (final_layer_norm): T5LayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (decoder): T5Stack(\n      (embed_tokens): Embedding(32128, 2048)\n      (block): ModuleList(\n        (0): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=2048, out_features=2048, bias=False)\n                (k): Linear(in_features=2048, out_features=2048, bias=False)\n                (v): Linear(in_features=2048, out_features=2048, bias=False)\n                (o): Linear(in_features=2048, out_features=2048, bias=False)\n                (relative_attention_bias): Embedding(32, 32)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerCrossAttention(\n              (EncDecAttention): T5Attention(\n                (q): Linear(in_features=2048, out_features=2048, bias=False)\n                (k): Linear(in_features=2048, out_features=2048, bias=False)\n                (v): Linear(in_features=2048, out_features=2048, bias=False)\n                (o): Linear(in_features=2048, out_features=2048, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (2): T5LayerFF(\n              (DenseReluDense): T5DenseGatedActDense(\n                (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n                (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n                (wo): Linear(in_features=5120, out_features=2048, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): GELUActivation()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (1-23): 23 x T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=2048, out_features=2048, bias=False)\n                (k): Linear(in_features=2048, out_features=2048, bias=False)\n                (v): Linear(in_features=2048, out_features=2048, bias=False)\n                (o): Linear(in_features=2048, out_features=2048, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerCrossAttention(\n              (EncDecAttention): T5Attention(\n                (q): Linear(in_features=2048, out_features=2048, bias=False)\n                (k): Linear(in_features=2048, out_features=2048, bias=False)\n                (v): Linear(in_features=2048, out_features=2048, bias=False)\n                (o): Linear(in_features=2048, out_features=2048, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (2): T5LayerFF(\n              (DenseReluDense): T5DenseGatedActDense(\n                (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n                (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n                (wo): Linear(in_features=5120, out_features=2048, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): GELUActivation()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (final_layer_norm): T5LayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (lm_head): Linear(in_features=2048, out_features=32128, bias=False)\n  )\n)"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"\n\n# VERIFY MODEL TYPE IMMEDIATELY AFTER LOADING\nprint(f\"Type of 'model' after loading: {type(model)}\")\nassert isinstance(model, Blip2ForConditionalGeneration), \"Model is not a Blip2ForConditionalGeneration instance!\"\n\npreds, refs = [], []\ncount  = 0\nwith torch.no_grad():\n    for idx, row in df.iterrows():\n        try:\n            img = Image.open(row.image_path).convert(\"RGB\")\n        except FileNotFoundError:\n            print(f\"Warning: Image not found at {row.image_path}, skipping row {idx}\")\n            preds.append(\"IMAGE_NOT_FOUND\")\n            refs.append(row.answer.strip() if pd.notna(row.answer) else \"\")\n            continue\n        except Exception as e:\n            print(f\"Error loading image {row.image_path}: {e}, skipping row {idx}\")\n            preds.append(\"IMAGE_ERROR\")\n            refs.append(row.answer.strip() if pd.notna(row.answer) else \"\")\n            continue\n\n        original_question = row.question\n        prompt = f\"Based on the image, answer the following question with a single word. Question: {original_question} Answer:\"\n\n\n        inputs = processor(images=img, text=prompt, return_tensors=\"pt\")\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n        generated_ids = model.generate(\n            **inputs,\n            max_new_tokens=10,   # Try values like 5, 10.\n            # num_beams=1,         # Optional: for greedy decoding, sometimes more direct\n            # do_sample=False      # Optional: turn off sampling\n        )\n        pred_full = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n\n        \n        words = pred_full.split()\n        if words:\n            pred = words[0] # Take the first word\n            pred = pred.rstrip('.,;:!?') # Remove common trailing punctuation\n        else:\n            pred = \"\" # Handle empty prediction\n\n        print(f\"\\nIdx: {idx}\")\n        print(f\"Original Question: {original_question}\")\n        print(f\"Prompt Used: '{prompt}'\")\n        print(f\"Full Prediction: '{pred_full}'\")\n        print(f\"Processed Prediction (Single Word Attempt): '{pred}'\")\n\n        count +=1\n        preds.append(pred)\n        refs.append(row.answer.strip() if pd.notna(row.answer) else \"\")\n        print(\"count: \", count)\n        # if (count == 100): # Limiting to 100 for the example\n        #     print(\"\\nReached count limit (100). Breaking loop.\")\n        #     break\n\nprint(\"length of predictions\", len(preds))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T20:33:41.031397Z","iopub.execute_input":"2025-05-10T20:33:41.032020Z","iopub.status.idle":"2025-05-10T20:34:07.026269Z","shell.execute_reply.started":"2025-05-10T20:33:41.031994Z","shell.execute_reply":"2025-05-10T20:34:07.025659Z"}},"outputs":[{"name":"stdout","text":"Type of 'model' after loading: <class 'transformers.models.blip_2.modeling_blip_2.Blip2ForConditionalGeneration'>\n\nIdx: 0\nOriginal Question: What is the main color of the shoe?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What is the main color of the shoe? Answer:'\nFull Prediction: 'brown'\nProcessed Prediction (Single Word Attempt): 'brown'\ncount:  1\n\nIdx: 1\nOriginal Question: What is the pattern on the shoe?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What is the pattern on the shoe? Answer:'\nFull Prediction: 'python'\nProcessed Prediction (Single Word Attempt): 'python'\ncount:  2\n\nIdx: 2\nOriginal Question: Does the shoe have a tassel?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: Does the shoe have a tassel? Answer:'\nFull Prediction: 'yes'\nProcessed Prediction (Single Word Attempt): 'yes'\ncount:  3\n\nIdx: 3\nOriginal Question: What color are the drawer slides?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What color are the drawer slides? Answer:'\nFull Prediction: 'white'\nProcessed Prediction (Single Word Attempt): 'white'\ncount:  4\n\nIdx: 4\nOriginal Question: What appears to be the material of the slides?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What appears to be the material of the slides? Answer:'\nFull Prediction: 'white'\nProcessed Prediction (Single Word Attempt): 'white'\ncount:  5\n\nIdx: 5\nOriginal Question: What shape are the screws?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What shape are the screws? Answer:'\nFull Prediction: 'screw'\nProcessed Prediction (Single Word Attempt): 'screw'\ncount:  6\n\nIdx: 6\nOriginal Question: What color is the filament?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What color is the filament? Answer:'\nFull Prediction: 'yellow'\nProcessed Prediction (Single Word Attempt): 'yellow'\ncount:  7\n\nIdx: 7\nOriginal Question: What is the shape of the spool?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What is the shape of the spool? Answer:'\nFull Prediction: 'spool'\nProcessed Prediction (Single Word Attempt): 'spool'\ncount:  8\n\nIdx: 8\nOriginal Question: Is the spool primarily black and orange?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: Is the spool primarily black and orange? Answer:'\nFull Prediction: 'yes'\nProcessed Prediction (Single Word Attempt): 'yes'\ncount:  9\n\nIdx: 9\nOriginal Question: What is the primary color of the fabric?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What is the primary color of the fabric? Answer:'\nFull Prediction: 'grey'\nProcessed Prediction (Single Word Attempt): 'grey'\ncount:  10\n\nIdx: 10\nOriginal Question: Does the fabric appear to be smooth?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: Does the fabric appear to be smooth? Answer:'\nFull Prediction: 'yes'\nProcessed Prediction (Single Word Attempt): 'yes'\ncount:  11\n\nIdx: 11\nOriginal Question: What is the apparent texture of the material?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What is the apparent texture of the material? Answer:'\nFull Prediction: 'a smooth surface'\nProcessed Prediction (Single Word Attempt): 'a'\ncount:  12\n\nIdx: 12\nOriginal Question: What color is the shoe?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What color is the shoe? Answer:'\nFull Prediction: 'tan'\nProcessed Prediction (Single Word Attempt): 'tan'\ncount:  13\n\nIdx: 13\nOriginal Question: What is the apparent material of the shoe?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What is the apparent material of the shoe? Answer:'\nFull Prediction: 'suede'\nProcessed Prediction (Single Word Attempt): 'suede'\ncount:  14\n\nIdx: 14\nOriginal Question: Is the shoe a loafer style?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: Is the shoe a loafer style? Answer:'\nFull Prediction: 'yes'\nProcessed Prediction (Single Word Attempt): 'yes'\ncount:  15\n\nIdx: 15\nOriginal Question: What color is the middle stripe?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What color is the middle stripe? Answer:'\nFull Prediction: 'red'\nProcessed Prediction (Single Word Attempt): 'red'\ncount:  16\n\nIdx: 16\nOriginal Question: What is the overall shape of the item?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What is the overall shape of the item? Answer:'\nFull Prediction: 'spherical'\nProcessed Prediction (Single Word Attempt): 'spherical'\ncount:  17\n\nIdx: 17\nOriginal Question: Is the top stripe white?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: Is the top stripe white? Answer:'\nFull Prediction: 'no'\nProcessed Prediction (Single Word Attempt): 'no'\ncount:  18\n\nIdx: 18\nOriginal Question: What color are the butterflies?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What color are the butterflies? Answer:'\nFull Prediction: 'white'\nProcessed Prediction (Single Word Attempt): 'white'\ncount:  19\n\nIdx: 19\nOriginal Question: What is the background color of the case?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What is the background color of the case? Answer:'\nFull Prediction: 'white'\nProcessed Prediction (Single Word Attempt): 'white'\ncount:  20\n\nIdx: 20\nOriginal Question: Is the phone case transparent?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: Is the phone case transparent? Answer:'\nFull Prediction: 'no'\nProcessed Prediction (Single Word Attempt): 'no'\ncount:  21\n\nIdx: 21\nOriginal Question: What is the apparent shape of the glass?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What is the apparent shape of the glass? Answer:'\nFull Prediction: 'square'\nProcessed Prediction (Single Word Attempt): 'square'\ncount:  22\n\nIdx: 22\nOriginal Question: What color is the glass?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What color is the glass? Answer:'\nFull Prediction: 'clear'\nProcessed Prediction (Single Word Attempt): 'clear'\ncount:  23\n\nIdx: 23\nOriginal Question: Is the glass a single or double old fashioned glass?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: Is the glass a single or double old fashioned glass? Answer:'\nFull Prediction: 'double'\nProcessed Prediction (Single Word Attempt): 'double'\ncount:  24\n\nIdx: 24\nOriginal Question: What color are the butterflies?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What color are the butterflies? Answer:'\nFull Prediction: 'blue'\nProcessed Prediction (Single Word Attempt): 'blue'\ncount:  25\n\nIdx: 25\nOriginal Question: What is the background color?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What is the background color? Answer:'\nFull Prediction: 'black'\nProcessed Prediction (Single Word Attempt): 'black'\ncount:  26\n\nIdx: 26\nOriginal Question: What is the main design element?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What is the main design element? Answer:'\nFull Prediction: 'a butterfly'\nProcessed Prediction (Single Word Attempt): 'a'\ncount:  27\n\nIdx: 27\nOriginal Question: What color is the chair cover?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What color is the chair cover? Answer:'\nFull Prediction: 'green'\nProcessed Prediction (Single Word Attempt): 'green'\ncount:  28\n\nIdx: 28\nOriginal Question: What is the apparent material of the chair cover?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What is the apparent material of the chair cover? Answer:'\nFull Prediction: 'velvet'\nProcessed Prediction (Single Word Attempt): 'velvet'\ncount:  29\n\nIdx: 29\nOriginal Question: Is the chair cover on a wooden chair?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: Is the chair cover on a wooden chair? Answer:'\nFull Prediction: 'no'\nProcessed Prediction (Single Word Attempt): 'no'\ncount:  30\n\nIdx: 30\nOriginal Question: What color are the shoes?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What color are the shoes? Answer:'\nFull Prediction: 'tan'\nProcessed Prediction (Single Word Attempt): 'tan'\ncount:  31\n\nIdx: 31\nOriginal Question: What type of shoe is shown?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What type of shoe is shown? Answer:'\nFull Prediction: 'loafer'\nProcessed Prediction (Single Word Attempt): 'loafer'\ncount:  32\n\nIdx: 32\nOriginal Question: What material are the shoes seemingly made of?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What material are the shoes seemingly made of? Answer:'\nFull Prediction: 'leather'\nProcessed Prediction (Single Word Attempt): 'leather'\ncount:  33\n\nIdx: 33\nOriginal Question: What is the dominant color on the bottom band?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What is the dominant color on the bottom band? Answer:'\nFull Prediction: 'red'\nProcessed Prediction (Single Word Attempt): 'red'\ncount:  34\n\nIdx: 34\nOriginal Question: What is the shape of the phone case?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What is the shape of the phone case? Answer:'\nFull Prediction: 'st'\nProcessed Prediction (Single Word Attempt): 'st'\ncount:  35\n\nIdx: 35\nOriginal Question: Is the case transparent?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: Is the case transparent? Answer:'\nFull Prediction: 'no'\nProcessed Prediction (Single Word Attempt): 'no'\ncount:  36\n\nIdx: 36\nOriginal Question: What color is the dominant background?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What color is the dominant background? Answer:'\nFull Prediction: 'red'\nProcessed Prediction (Single Word Attempt): 'red'\ncount:  37\n\nIdx: 37\nOriginal Question: What shape is the phone case?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What shape is the phone case? Answer:'\nFull Prediction: 'red'\nProcessed Prediction (Single Word Attempt): 'red'\ncount:  38\n\nIdx: 38\nOriginal Question: Is the case transparent?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: Is the case transparent? Answer:'\nFull Prediction: 'no'\nProcessed Prediction (Single Word Attempt): 'no'\ncount:  39\n\nIdx: 39\nOriginal Question: What color are most of the cacti?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What color are most of the cacti? Answer:'\nFull Prediction: 'green'\nProcessed Prediction (Single Word Attempt): 'green'\ncount:  40\n\nIdx: 40\nOriginal Question: Is there a flower on the cacti?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: Is there a flower on the cacti? Answer:'\nFull Prediction: 'yes'\nProcessed Prediction (Single Word Attempt): 'yes'\ncount:  41\n\nIdx: 41\nOriginal Question: What is the overall shape of the phone case?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What is the overall shape of the phone case? Answer:'\nFull Prediction: 'cactus'\nProcessed Prediction (Single Word Attempt): 'cactus'\ncount:  42\n\nIdx: 42\nOriginal Question: What color is the background of the phone case?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What color is the background of the phone case? Answer:'\nFull Prediction: 'pink'\nProcessed Prediction (Single Word Attempt): 'pink'\ncount:  43\n\nIdx: 43\nOriginal Question: What is the color of the cat on the phone case?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What is the color of the cat on the phone case? Answer:'\nFull Prediction: 'black'\nProcessed Prediction (Single Word Attempt): 'black'\ncount:  44\n\nIdx: 44\nOriginal Question: What shape is the phone case?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What shape is the phone case? Answer:'\nFull Prediction: 'cat'\nProcessed Prediction (Single Word Attempt): 'cat'\ncount:  45\n\nIdx: 45\nOriginal Question: What color is the Eiffel Tower?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What color is the Eiffel Tower? Answer:'\nFull Prediction: 'red'\nProcessed Prediction (Single Word Attempt): 'red'\ncount:  46\n\nIdx: 46\nOriginal Question: What is the main shape used in the background?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What is the main shape used in the background? Answer:'\nFull Prediction: 'eiffel tower'\nProcessed Prediction (Single Word Attempt): 'eiffel'\ncount:  47\n\nIdx: 47\nOriginal Question: Is Big Ben present in the image?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: Is Big Ben present in the image? Answer:'\nFull Prediction: 'yes'\nProcessed Prediction (Single Word Attempt): 'yes'\ncount:  48\n\nIdx: 48\nOriginal Question: What color are the cacti?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What color are the cacti? Answer:'\nFull Prediction: 'green'\nProcessed Prediction (Single Word Attempt): 'green'\ncount:  49\n\nIdx: 49\nOriginal Question: What is the main subject of the image?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What is the main subject of the image? Answer:'\nFull Prediction: 'cactus'\nProcessed Prediction (Single Word Attempt): 'cactus'\ncount:  50\n\nIdx: 50\nOriginal Question: Is the phone case design predominantly green?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: Is the phone case design predominantly green? Answer:'\nFull Prediction: 'yes'\nProcessed Prediction (Single Word Attempt): 'yes'\ncount:  51\n\nIdx: 51\nOriginal Question: What color is the cup?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What color is the cup? Answer:'\nFull Prediction: 'blue'\nProcessed Prediction (Single Word Attempt): 'blue'\ncount:  52\n\nIdx: 52\nOriginal Question: What shape is the cup?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What shape is the cup? Answer:'\nFull Prediction: 'cup'\nProcessed Prediction (Single Word Attempt): 'cup'\ncount:  53\n\nIdx: 53\nOriginal Question: What is the apparent material of the phone case?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What is the apparent material of the phone case? Answer:'\nFull Prediction: 'metal'\nProcessed Prediction (Single Word Attempt): 'metal'\ncount:  54\n\nIdx: 54\nOriginal Question: What is the overall shape of the gemstone?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What is the overall shape of the gemstone? Answer:'\nFull Prediction: 'square'\nProcessed Prediction (Single Word Attempt): 'square'\ncount:  55\n\nIdx: 55\nOriginal Question: What appears to be the metal color?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What appears to be the metal color? Answer:'\nFull Prediction: 'white'\nProcessed Prediction (Single Word Attempt): 'white'\ncount:  56\n\nIdx: 56\nOriginal Question: What type of earring is shown?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What type of earring is shown? Answer:'\nFull Prediction: 'stud'\nProcessed Prediction (Single Word Attempt): 'stud'\ncount:  57\n\nIdx: 57\nOriginal Question: What color are the tulips?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What color are the tulips? Answer:'\nFull Prediction: 'blue'\nProcessed Prediction (Single Word Attempt): 'blue'\ncount:  58\n\nIdx: 58\nOriginal Question: What is the background color of the phone case?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What is the background color of the phone case? Answer:'\nFull Prediction: 'blue'\nProcessed Prediction (Single Word Attempt): 'blue'\ncount:  59\n\nIdx: 59\nOriginal Question: Is the phone case design floral?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: Is the phone case design floral? Answer:'\nFull Prediction: 'yes'\nProcessed Prediction (Single Word Attempt): 'yes'\ncount:  60\n\nIdx: 60\nOriginal Question: What colors are used in the design?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What colors are used in the design? Answer:'\nFull Prediction: 'black and white'\nProcessed Prediction (Single Word Attempt): 'black'\ncount:  61\n\nIdx: 61\nOriginal Question: What is the overall shape of the phone case?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What is the overall shape of the phone case? Answer:'\nFull Prediction: 'black'\nProcessed Prediction (Single Word Attempt): 'black'\ncount:  62\n\nIdx: 62\nOriginal Question: Is the pattern geometric?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: Is the pattern geometric? Answer:'\nFull Prediction: 'yes'\nProcessed Prediction (Single Word Attempt): 'yes'\ncount:  63\n\nIdx: 63\nOriginal Question: What color is the door handle?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What color is the door handle? Answer:'\nFull Prediction: 'brass'\nProcessed Prediction (Single Word Attempt): 'brass'\ncount:  64\n\nIdx: 64\nOriginal Question: What is the apparent material of the handle?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What is the apparent material of the handle? Answer:'\nFull Prediction: 'brass'\nProcessed Prediction (Single Word Attempt): 'brass'\ncount:  65\n\nIdx: 65\nOriginal Question: Is the handle a lever type?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: Is the handle a lever type? Answer:'\nFull Prediction: 'yes'\nProcessed Prediction (Single Word Attempt): 'yes'\ncount:  66\n\nIdx: 66\nOriginal Question: What is the predominant color of the landscape?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What is the predominant color of the landscape? Answer:'\nFull Prediction: 'red'\nProcessed Prediction (Single Word Attempt): 'red'\ncount:  67\n\nIdx: 67\nOriginal Question: Is there any green visible in the image?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: Is there any green visible in the image? Answer:'\nFull Prediction: 'no'\nProcessed Prediction (Single Word Attempt): 'no'\ncount:  68\n\nIdx: 68\nOriginal Question: What is the overall shape of the framed image?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What is the overall shape of the framed image? Answer:'\nFull Prediction: 'a square'\nProcessed Prediction (Single Word Attempt): 'a'\ncount:  69\n\nIdx: 69\nOriginal Question: What is the dominant color of the phone case?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What is the dominant color of the phone case? Answer:'\nFull Prediction: 'black'\nProcessed Prediction (Single Word Attempt): 'black'\ncount:  70\n\nIdx: 70\nOriginal Question: What is the overall shape of the phone case?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What is the overall shape of the phone case? Answer:'\nFull Prediction: 'rounded'\nProcessed Prediction (Single Word Attempt): 'rounded'\ncount:  71\n\nIdx: 71\nOriginal Question: Does the phone case have a pattern?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: Does the phone case have a pattern? Answer:'\nFull Prediction: 'yes'\nProcessed Prediction (Single Word Attempt): 'yes'\ncount:  72\n\nIdx: 72\nOriginal Question: What color is the candle?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What color is the candle? Answer:'\nFull Prediction: 'red'\nProcessed Prediction (Single Word Attempt): 'red'\ncount:  73\n\nIdx: 73\nOriginal Question: What shape is the candle?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What shape is the candle? Answer:'\nFull Prediction: 'candle'\nProcessed Prediction (Single Word Attempt): 'candle'\ncount:  74\n\nIdx: 74\nOriginal Question: Is the candle lit?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: Is the candle lit? Answer:'\nFull Prediction: 'yes'\nProcessed Prediction (Single Word Attempt): 'yes'\ncount:  75\n\nIdx: 75\nOriginal Question: What color are the mountains?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What color are the mountains? Answer:'\nFull Prediction: 'blue'\nProcessed Prediction (Single Word Attempt): 'blue'\ncount:  76\n\nIdx: 76\nOriginal Question: What is the primary mode of transportation shown?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What is the primary mode of transportation shown? Answer:'\nFull Prediction: 'train'\nProcessed Prediction (Single Word Attempt): 'train'\ncount:  77\n\nIdx: 77\nOriginal Question: What is the shape of the bridge?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What is the shape of the bridge? Answer:'\nFull Prediction: 'a bridge'\nProcessed Prediction (Single Word Attempt): 'a'\ncount:  78\n\nIdx: 78\nOriginal Question: What color is the dominant background?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What color is the dominant background? Answer:'\nFull Prediction: 'pink'\nProcessed Prediction (Single Word Attempt): 'pink'\ncount:  79\n\nIdx: 79\nOriginal Question: What shape is the main heart?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What shape is the main heart? Answer:'\nFull Prediction: 'heart'\nProcessed Prediction (Single Word Attempt): 'heart'\ncount:  80\n\nIdx: 80\nOriginal Question: What is the predominant pattern on the background?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What is the predominant pattern on the background? Answer:'\nFull Prediction: 'pink'\nProcessed Prediction (Single Word Attempt): 'pink'\ncount:  81\n\nIdx: 81\nOriginal Question: What color are the flowers?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What color are the flowers? Answer:'\nFull Prediction: 'pink'\nProcessed Prediction (Single Word Attempt): 'pink'\ncount:  82\n\nIdx: 82\nOriginal Question: What is the background color?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What is the background color? Answer:'\nFull Prediction: 'black'\nProcessed Prediction (Single Word Attempt): 'black'\ncount:  83\n\nIdx: 83\nOriginal Question: Is the phone case design floral?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: Is the phone case design floral? Answer:'\nFull Prediction: 'yes'\nProcessed Prediction (Single Word Attempt): 'yes'\ncount:  84\n\nIdx: 84\nOriginal Question: What color is the background of the phone case?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What color is the background of the phone case? Answer:'\nFull Prediction: 'gold'\nProcessed Prediction (Single Word Attempt): 'gold'\ncount:  85\n\nIdx: 85\nOriginal Question: What is the dominant color of the design?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What is the dominant color of the design? Answer:'\nFull Prediction: 'gold'\nProcessed Prediction (Single Word Attempt): 'gold'\ncount:  86\n\nIdx: 86\nOriginal Question: What letter is prominently displayed?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What letter is prominently displayed? Answer:'\nFull Prediction: 'c'\nProcessed Prediction (Single Word Attempt): 'c'\ncount:  87\n\nIdx: 87\nOriginal Question: What is the dominant color in the phone case?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What is the dominant color in the phone case? Answer:'\nFull Prediction: 'orange'\nProcessed Prediction (Single Word Attempt): 'orange'\ncount:  88\n\nIdx: 88\nOriginal Question: Is the phone case design abstract?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: Is the phone case design abstract? Answer:'\nFull Prediction: 'yes'\nProcessed Prediction (Single Word Attempt): 'yes'\ncount:  89\n\nIdx: 89\nOriginal Question: What kind of material does the phone case appear to be?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What kind of material does the phone case appear to be? Answer:'\nFull Prediction: 'acrylic'\nProcessed Prediction (Single Word Attempt): 'acrylic'\ncount:  90\n\nIdx: 90\nOriginal Question: What is the predominant color of the design?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What is the predominant color of the design? Answer:'\nFull Prediction: 'red'\nProcessed Prediction (Single Word Attempt): 'red'\ncount:  91\n\nIdx: 91\nOriginal Question: What is the apparent material of the item?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What is the apparent material of the item? Answer:'\nFull Prediction: 'fabric'\nProcessed Prediction (Single Word Attempt): 'fabric'\ncount:  92\n\nIdx: 92\nOriginal Question: Is the pattern floral?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: Is the pattern floral? Answer:'\nFull Prediction: 'yes'\nProcessed Prediction (Single Word Attempt): 'yes'\ncount:  93\n\nIdx: 93\nOriginal Question: What color is the background of the phone case?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What color is the background of the phone case? Answer:'\nFull Prediction: 'black'\nProcessed Prediction (Single Word Attempt): 'black'\ncount:  94\n\nIdx: 94\nOriginal Question: What is the shape of the phone case?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What is the shape of the phone case? Answer:'\nFull Prediction: 'never be alone'\nProcessed Prediction (Single Word Attempt): 'never'\ncount:  95\n\nIdx: 95\nOriginal Question: Is there a moon on the phone case?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: Is there a moon on the phone case? Answer:'\nFull Prediction: 'yes'\nProcessed Prediction (Single Word Attempt): 'yes'\ncount:  96\n\nIdx: 96\nOriginal Question: What color is the pump?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What color is the pump? Answer:'\nFull Prediction: 'black'\nProcessed Prediction (Single Word Attempt): 'black'\ncount:  97\n\nIdx: 97\nOriginal Question: What is the apparent material of the pump?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What is the apparent material of the pump? Answer:'\nFull Prediction: 'black'\nProcessed Prediction (Single Word Attempt): 'black'\ncount:  98\n\nIdx: 98\nOriginal Question: What shape is the float?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What shape is the float? Answer:'\nFull Prediction: 'spherical'\nProcessed Prediction (Single Word Attempt): 'spherical'\ncount:  99\n\nIdx: 99\nOriginal Question: What color is the central strip on the back support?\nPrompt Used: 'Based on the image, answer the following question with a single word. Question: What color is the central strip on the back support? Answer:'\nFull Prediction: 'black'\nProcessed Prediction (Single Word Attempt): 'black'\ncount:  100\n\nReached count limit (100). Breaking loop.\nlength of predictions 100\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"! pip install git+https://github.com/google-research/bleurt.git\n!pip install scikit-learn evaluate bert-score rouge-score \\\n            sentence-transformers rapidfuzz\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T20:32:43.279812Z","iopub.execute_input":"2025-05-10T20:32:43.280540Z","iopub.status.idle":"2025-05-10T20:32:58.284808Z","shell.execute_reply.started":"2025-05-10T20:32:43.280512Z","shell.execute_reply":"2025-05-10T20:32:58.283814Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting git+https://github.com/google-research/bleurt.git\n  Cloning https://github.com/google-research/bleurt.git to /tmp/pip-req-build-jwbpyhym\n  Running command git clone --filter=blob:none --quiet https://github.com/google-research/bleurt.git /tmp/pip-req-build-jwbpyhym\n  Resolved https://github.com/google-research/bleurt.git to commit cebe7e6f996b40910cfaa520a63db47807e3bf5c\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from BLEURT==0.0.2) (2.2.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from BLEURT==0.0.2) (1.26.4)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from BLEURT==0.0.2) (1.15.2)\nRequirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (from BLEURT==0.0.2) (2.18.0)\nRequirement already satisfied: tf-slim>=1.1 in /usr/local/lib/python3.11/dist-packages (from BLEURT==0.0.2) (1.1.0)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from BLEURT==0.0.2) (0.2.0)\nRequirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.11/dist-packages (from tf-slim>=1.1->BLEURT==0.0.2) (1.4.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->BLEURT==0.0.2) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->BLEURT==0.0.2) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->BLEURT==0.0.2) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->BLEURT==0.0.2) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->BLEURT==0.0.2) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->BLEURT==0.0.2) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->BLEURT==0.0.2) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->BLEURT==0.0.2) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->BLEURT==0.0.2) (2025.2)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (1.6.3)\nRequirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (25.2.10)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (0.6.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (0.2.0)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (18.1.1)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (3.4.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (24.2)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (2.32.3)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (75.1.0)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (1.17.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (2.5.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (4.13.1)\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (1.17.2)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (1.70.0)\nRequirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (2.18.0)\nRequirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (3.5.0)\nRequirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (3.12.1)\nRequirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (0.4.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (0.37.1)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow->BLEURT==0.0.2) (0.45.1)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow->BLEURT==0.0.2) (14.0.0)\nRequirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow->BLEURT==0.0.2) (0.0.8)\nRequirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow->BLEURT==0.0.2) (0.14.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->BLEURT==0.0.2) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->BLEURT==0.0.2) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->BLEURT==0.0.2) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->BLEURT==0.0.2) (2025.1.31)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->BLEURT==0.0.2) (3.7)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->BLEURT==0.0.2) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->BLEURT==0.0.2) (3.1.3)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->BLEURT==0.0.2) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->BLEURT==0.0.2) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->BLEURT==0.0.2) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->BLEURT==0.0.2) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->BLEURT==0.0.2) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow->BLEURT==0.0.2) (3.0.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow->BLEURT==0.0.2) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow->BLEURT==0.0.2) (2.19.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow->BLEURT==0.0.2) (0.1.2)\nBuilding wheels for collected packages: BLEURT\n  Building wheel for BLEURT (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for BLEURT: filename=BLEURT-0.0.2-py3-none-any.whl size=16456764 sha256=a01fa010d2f4fd559dbb5995687041f0ec520ab8cfb5d90f57b4a5d1721c5ea3\n  Stored in directory: /tmp/pip-ephem-wheel-cache-o38tqfwk/wheels/30/af/34/e148007788b060e4c76e7ecf68e70c692dff0f2632e62ac454\nSuccessfully built BLEURT\nInstalling collected packages: BLEURT\nSuccessfully installed BLEURT-0.0.2\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\nCollecting bert-score\n  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\nCollecting rouge-score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\nCollecting rapidfuzz\n  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.30.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\nRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.5.1+cu124)\nRequirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.51.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert-score) (3.7.5)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.16)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert-score) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.5.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.4.8)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (3.2.1)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.1.8)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.19.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17.3->scikit-learn) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17.3->scikit-learn) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17.3->scikit-learn) (2024.2.0)\nDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: rouge-score\n  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=61acd9476d7a5e1cdbc91f18735f85e0976946c6285781eefe581348667343f9\n  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\nSuccessfully built rouge-score\nInstalling collected packages: rapidfuzz, rouge-score, bert-score\nSuccessfully installed bert-score-0.3.13 rapidfuzz-3.13.0 rouge-score-0.1.2\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# ──────────────────────────────────────────────────────────────────────────────\n# 6. Compute Metrics (and identify mismatches)\n# ──────────────────────────────────────────────────────────────────────────────\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nimport evaluate\n# from bert_score import BERTScorer # Keep if you add BERTScore later\n# from rouge_score import rouge_scorer # Keep if you add ROUGE later\n# from rapidfuzz.distance import Levenshtein # Keep if you add Levenshtein later\n# from sentence_transformers import SentenceTransformer, util # Keep if you add SBERT later\nimport numpy as np\nimport pandas as pd # Ensure pandas is imported if df is used\n\n\n\n# normalize case\npreds_l = [str(p).lower().strip() for p in preds] # Added str() and strip() for robustness\nrefs_l  = [str(r).lower().strip() for r in refs]   # Added str() and strip()\n\n# 1) TOKEN-LEVEL EXACT MATCH → binary labels\ny_pred_bin = [int(p == r) for p, r in zip(preds_l, refs_l)]\ny_true_bin = [1]*len(refs_l)            # reference is always “correct” class\n\nacc = accuracy_score(y_true_bin, y_pred_bin)\nprec, rec, f1, _ = precision_recall_fscore_support(\n    y_true_bin, y_pred_bin, average=\"binary\", zero_division=0\n)\nprint(f\"Exact-match Accuracy: {acc:.3f}\")\nprint(f\"Exact-match Precision: {prec:.3f}\") # Will be 1.0 if acc > 0, else 0.0\nprint(f\"Exact-match Recall:    {rec:.3f}\")    # Will be same as acc\nprint(f\"Exact-match F1:        {f1:.3f}\\n\")  # Will be same as acc\n\n# ──────────────────────────────────────────────────────────────────────────────\n# 7. Identify and Print Mismatched Predictions\n# ──────────────────────────────────────────────────────────────────────────────\nprint(\"\\n--- Mismatched Predictions ---\")\nmismatch_count = 0\n# We iterate up to the length of preds_l, which is derived from 'preds'\n# This assumes 'preds', 'refs', and the relevant part of 'df' are aligned\nfor i in range(len(preds_l)):\n    prediction_normalized = preds_l[i]\n    reference_normalized = refs_l[i]\n\n    if prediction_normalized != reference_normalized:\n        mismatch_count += 1\n        print(f\"\\nMismatch #{mismatch_count} (Original Index: {i})\")\n\n        # Try to get the original question from the DataFrame\n        # This assumes 'df' is available and 'i' corresponds to the row index used during inference.\n        # If you only processed a subset of df (e.g., first 5 rows), this will work for i < 5.\n        try:\n            # If your inference loop used df.iterrows(), 'i' corresponds to the iteration count,\n            # which might be the same as df.iloc[i] if you iterated from the start of df.\n            # If you used a different way to get 'row' in your inference, adjust access to df.\n            question = df.iloc[i]['question'] # Assuming 'preds' came from iterating 'df' from the start\n            print(f\"  Question: \\\"{question}\\\"\")\n        except (NameError, KeyError, IndexError) as e:\n            # NameError if df is not defined\n            # KeyError if 'question' column doesn't exist\n            # IndexError if i is out of bounds for df (e.g., if df is smaller than len(preds_l))\n            print(f\"  Question: (Could not retrieve - {type(e).__name__})\")\n\n        print(f\"  Model Predicted: \\\"{preds[i]}\\\" (Normalized: \\\"{prediction_normalized}\\\")\")\n        print(f\"  Correct Answer:  \\\"{refs[i]}\\\" (Normalized: \\\"{reference_normalized}\\\")\")\n        print(\"-\" * 30)\n\n    if mismatch_count == 15:\n        break\n\nif mismatch_count == 0 and len(preds_l) > 0:\n    print(\"No mismatches found! All (normalized) predictions exactly matched their references.\")\nelif len(preds_l) == 0:\n    print(\"No predictions to compare.\")\n\n# Now you can add the other metrics calculation code (ROUGE, BERTScore, etc.) here if you wish","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T20:39:08.963677Z","iopub.execute_input":"2025-05-10T20:39:08.964433Z","iopub.status.idle":"2025-05-10T20:39:08.977888Z","shell.execute_reply.started":"2025-05-10T20:39:08.964389Z","shell.execute_reply":"2025-05-10T20:39:08.977152Z"}},"outputs":[{"name":"stdout","text":"Exact-match Accuracy: 0.500\nExact-match Precision: 1.000\nExact-match Recall:    0.500\nExact-match F1:        0.667\n\n\n--- Mismatched Predictions ---\n\nMismatch #1 (Original Index: 1)\n  Question: \"What is the pattern on the shoe?\"\n  Model Predicted: \"python\" (Normalized: \"python\")\n  Correct Answer:  \"Snakeskin\" (Normalized: \"snakeskin\")\n------------------------------\n\nMismatch #2 (Original Index: 4)\n  Question: \"What appears to be the material of the slides?\"\n  Model Predicted: \"white\" (Normalized: \"white\")\n  Correct Answer:  \"Metal\" (Normalized: \"metal\")\n------------------------------\n\nMismatch #3 (Original Index: 5)\n  Question: \"What shape are the screws?\"\n  Model Predicted: \"screw\" (Normalized: \"screw\")\n  Correct Answer:  \"Phillips\" (Normalized: \"phillips\")\n------------------------------\n\nMismatch #4 (Original Index: 7)\n  Question: \"What is the shape of the spool?\"\n  Model Predicted: \"spool\" (Normalized: \"spool\")\n  Correct Answer:  \"Round\" (Normalized: \"round\")\n------------------------------\n\nMismatch #5 (Original Index: 9)\n  Question: \"What is the primary color of the fabric?\"\n  Model Predicted: \"grey\" (Normalized: \"grey\")\n  Correct Answer:  \"Gray\" (Normalized: \"gray\")\n------------------------------\n\nMismatch #6 (Original Index: 11)\n  Question: \"What is the apparent texture of the material?\"\n  Model Predicted: \"a\" (Normalized: \"a\")\n  Correct Answer:  \"Woven\" (Normalized: \"woven\")\n------------------------------\n\nMismatch #7 (Original Index: 12)\n  Question: \"What color is the shoe?\"\n  Model Predicted: \"tan\" (Normalized: \"tan\")\n  Correct Answer:  \"Beige\" (Normalized: \"beige\")\n------------------------------\n\nMismatch #8 (Original Index: 15)\n  Question: \"What color is the middle stripe?\"\n  Model Predicted: \"red\" (Normalized: \"red\")\n  Correct Answer:  \"Blue\" (Normalized: \"blue\")\n------------------------------\n\nMismatch #9 (Original Index: 16)\n  Question: \"What is the overall shape of the item?\"\n  Model Predicted: \"spherical\" (Normalized: \"spherical\")\n  Correct Answer:  \"Rectangle\" (Normalized: \"rectangle\")\n------------------------------\n\nMismatch #10 (Original Index: 17)\n  Question: \"Is the top stripe white?\"\n  Model Predicted: \"no\" (Normalized: \"no\")\n  Correct Answer:  \"Yes\" (Normalized: \"yes\")\n------------------------------\n\nMismatch #11 (Original Index: 18)\n  Question: \"What color are the butterflies?\"\n  Model Predicted: \"white\" (Normalized: \"white\")\n  Correct Answer:  \"Blue\" (Normalized: \"blue\")\n------------------------------\n\nMismatch #12 (Original Index: 20)\n  Question: \"Is the phone case transparent?\"\n  Model Predicted: \"no\" (Normalized: \"no\")\n  Correct Answer:  \"Yes\" (Normalized: \"yes\")\n------------------------------\n\nMismatch #13 (Original Index: 24)\n  Question: \"What color are the butterflies?\"\n  Model Predicted: \"blue\" (Normalized: \"blue\")\n  Correct Answer:  \"Pink\" (Normalized: \"pink\")\n------------------------------\n\nMismatch #14 (Original Index: 26)\n  Question: \"What is the main design element?\"\n  Model Predicted: \"a\" (Normalized: \"a\")\n  Correct Answer:  \"Butterflies\" (Normalized: \"butterflies\")\n------------------------------\n\nMismatch #15 (Original Index: 29)\n  Question: \"Is the chair cover on a wooden chair?\"\n  Model Predicted: \"no\" (Normalized: \"no\")\n  Correct Answer:  \"Yes\" (Normalized: \"yes\")\n------------------------------\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# --- ROUGE ---\nprint(\"\\n--- ROUGE Scores ---\")\nrouge_eval_scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\nrouge1_scores = []\nrougeL_scores = []\nfor pred, ref in zip(preds_l, refs_l):\n    if not pred or not ref: # Handle empty strings if any\n        rouge1_scores.append(0.0)\n        rougeL_scores.append(0.0)\n        continue\n    scores = rouge_eval_scorer.score(ref, pred) # Target, Prediction\n    rouge1_scores.append(scores['rouge1'].fmeasure)\n    rougeL_scores.append(scores['rougeL'].fmeasure)\n\nif rouge1_scores:\n    print(f\"Average ROUGE-1 F1: {np.mean(rouge1_scores):.3f}\")\n    print(f\"Average ROUGE-L F1: {np.mean(rougeL_scores):.3f}\")\nelse:\n    print(\"No ROUGE scores to compute (empty predictions or references).\")\n\n\n# --- BERTScore ---\nprint(\"\\n--- BERTScore ---\")\ntry:\n    bert_eval_scorer = BERTScorer(lang=\"en\", rescale_with_baseline=True, device=device)\n    filtered_preds_l = [p for p, r in zip(preds_l, refs_l) if p and r]\n    filtered_refs_l = [r for p, r in zip(preds_l, refs_l) if p and r]\n\n    if filtered_preds_l and filtered_refs_l:\n        P, R, F1_bert = bert_eval_scorer.score(filtered_preds_l, filtered_refs_l)\n        print(f\"Average BERTScore Precision: {P.mean():.3f}\")\n        print(f\"Average BERTScore Recall:    {R.mean():.3f}\")\n        print(f\"Average BERTScore F1:        {F1_bert.mean():.3f}\")\n    else:\n        print(\"Not enough valid (non-empty) prediction/reference pairs for BERTScore.\")\nexcept Exception as e:\n    print(f\"Could not compute BERTScore: {e}\")\n\n\n# --- Levenshtein Normalized Similarity ---\nprint(\"\\n--- Levenshtein Normalized Similarity ---\")\nlev_similarities = []\nfor pred, ref in zip(preds_l, refs_l):\n    if not pred and not ref: # both empty, perfect match\n        similarity = 1.0\n    elif not pred or not ref: # one empty, other not, zero similarity\n        similarity = 0.0\n    else:\n        similarity = Levenshtein.normalized_similarity(pred, ref)\n    lev_similarities.append(similarity)\n\nif lev_similarities:\n    print(f\"Average Levenshtein Normalized Similarity: {np.mean(lev_similarities):.3f}\")\nelse:\n    print(\"No Levenshtein similarities to compute.\")\n\n\n# --- Sentence Transformer Cosine Similarity ---\nprint(\"\\n--- Sentence-BERT Cosine Similarity ---\")\ntry:\n    sbert_model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n    valid_pairs_indices = [i for i, (p, r) in enumerate(zip(preds_l, refs_l)) if p and r]\n    sbert_preds = [preds_l[i] for i in valid_pairs_indices]\n    sbert_refs = [refs_l[i] for i in valid_pairs_indices]\n\n    if sbert_preds and sbert_refs:\n        embeddings_preds = sbert_model.encode(sbert_preds, convert_to_tensor=True)\n        embeddings_refs = sbert_model.encode(sbert_refs, convert_to_tensor=True)\n\n        cosine_scores_sbert = util.cos_sim(embeddings_preds, embeddings_refs)\n        pair_similarities = [cosine_scores_sbert[i, i].item() for i in range(len(sbert_preds))]\n        print(f\"Average Sentence-BERT Cosine Similarity: {np.mean(pair_similarities):.3f}\")\n    else:\n        print(\"Not enough valid (non-empty) prediction/reference pairs for Sentence-BERT similarity.\")\nexcept Exception as e:\n    print(f\"Could not compute Sentence-BERT similarity: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T20:34:48.879663Z","iopub.execute_input":"2025-05-10T20:34:48.879948Z","iopub.status.idle":"2025-05-10T20:34:51.224001Z","shell.execute_reply.started":"2025-05-10T20:34:48.879925Z","shell.execute_reply":"2025-05-10T20:34:51.223435Z"}},"outputs":[{"name":"stdout","text":"\n--- ROUGE Scores ---\nAverage ROUGE-1 F1: 0.510\nAverage ROUGE-L F1: 0.510\n\n--- BERTScore ---\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Average BERTScore Precision: 0.871\nAverage BERTScore Recall:    0.848\nAverage BERTScore F1:        0.858\n\n--- Levenshtein Normalized Similarity ---\nAverage Levenshtein Normalized Similarity: 0.557\n\n--- Sentence-BERT Cosine Similarity ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1503390064b041d6bc14565d28d0775a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"851559764e6047ef9a00364d0038cca6"}},"metadata":{}},{"name":"stdout","text":"Average Sentence-BERT Cosine Similarity: 0.749\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}